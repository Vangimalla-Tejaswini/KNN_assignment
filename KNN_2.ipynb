{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e97cbe3",
   "metadata": {},
   "source": [
    "#### Q1. What is the main difference between the Euclidean distance metric and the Manhattan distance metric in KNN? How might this difference affect the performance of a KNN classifier or regressor?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cddd570e",
   "metadata": {},
   "source": [
    "The main difference between the Euclidean distance metric and the Manhattan distance metric lies in how they calculate distance between points. Euclidean distance is the straight-line distance between two points in Euclidean space, calculated as the square root of the sum of the squared differences between corresponding coordinates. On the other hand, Manhattan distance, also known as city block distance or L1 distance, calculates distance by summing the absolute differences between corresponding coordinates. The choice between these metrics can affect KNN performance based on the distribution and nature of the data. For instance, in datasets where features are measured in different units or have varying scales, Euclidean distance might be sensitive to these differences and could lead to biased results. In such cases, Manhattan distance might offer a more robust alternative.\n",
    "\n",
    "Euclidean distance between point\n",
    "\n",
    "Manhattan distance between points (x1,y1),(x2,y2)\n",
    "\n",
    "sqrt[(y2-y1)*2+(x2-x1)*2]:\n",
    "\n",
    "Manhattan distancebetween points (x1,y1),(x2,y2)\n",
    "\n",
    "Manhattan distance=∣x2−x1∣+∣y2−y1∣"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d2feb7",
   "metadata": {},
   "source": [
    "#### Q2. How do you choose the optimal value of k for a KNN classifier or regressor? What techniques can be used to determine the optimal k value?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de9dbd6",
   "metadata": {},
   "source": [
    "Selecting the optimal value of k in KNN involves balancing bias and variance. A smaller value of k leads to a more flexible model with lower bias but higher variance, potentially resulting in overfitting. Conversely, a larger value of k reduces variance but may introduce higher bias. Techniques for determining the optimal k value include cross-validation, where the dataset is split into training and validation sets multiple times to evaluate performance for different values of k, and using techniques such as grid search or randomized search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baff370d",
   "metadata": {},
   "source": [
    "#### Q3. How does the choice of distance metric affect the performance of a KNN classifier or regressor? In what situations might you choose one distance metric over the other?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0965d1c5",
   "metadata": {},
   "source": [
    "The choice of distance metric significantly influences KNN performance. Euclidean distance is commonly used when the data attributes are continuous and have a linear relationship. However, if the dataset contains categorical or ordinal variables, Manhattan distance might be more appropriate. Additionally, Manhattan distance is less sensitive to outliers compared to Euclidean distance, making it a better choice when dealing with noisy data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee5670c",
   "metadata": {},
   "source": [
    "#### Q4. What are some common hyperparameters in KNN classifiers and regressors, and how do they affect the performance of the model? How might you go about tuning these hyperparameters to improve model performance?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b14899",
   "metadata": {},
   "source": [
    "Common hyperparameters in KNN classifiers and regressors include k (number of neighbors), the choice of distance metric (e.g., Euclidean or Manhattan), and optional parameters such as weights (uniform or distance-based) and algorithm optimization techniques (e.g., KD-tree or Ball-tree). Tuning these hyperparameters involves using techniques like grid search or randomized search over a predefined range of values while evaluating model performance using cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d015bcdc",
   "metadata": {},
   "source": [
    "#### Q5. How does the size of the training set affect the performance of a KNN classifier or regressor? What techniques can be used to optimize the size of the training set?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa89395b",
   "metadata": {},
   "source": [
    " The size of the training set can impact KNN performance. A larger training set can provide a better representation of the underlying data distribution, potentially leading to improved model generalization. However, excessively large training sets might introduce computational overhead and slow down the KNN algorithm. Techniques such as cross-validation and learning curves can be employed to optimize the size of the training set by evaluating model performance on subsets of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d785ed1a",
   "metadata": {},
   "source": [
    "#### Q6. What are some potential drawbacks of using KNN as a classifier or regressor? How might you overcome these drawbacks to improve the performance of the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efd4267",
   "metadata": {},
   "source": [
    "Some potential drawbacks of using KNN include its sensitivity to irrelevant features, high computational cost during prediction (especially with large datasets), and the need to store the entire training dataset in memory for prediction. To mitigate these drawbacks, feature selection techniques can be applied to remove irrelevant features, dimensionality reduction methods like Principal Component Analysis (PCA) can be used to reduce computational complexity, and approximate nearest neighbor algorithms can be employed to speed up the prediction process while reducing memory requirements. Additionally, preprocessing techniques such as feature scaling can improve the performance of KNN by ensuring that all features contribute equally to distance calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e1648d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
